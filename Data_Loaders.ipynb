{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMOw4Zm7UpQ9Wjcg28jz+Wz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahilmaniyar888/Neural-Network-from-Scratch/blob/main/Data_Loaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieD3Ya38Bfed",
        "outputId": "edf457a6-05ff-438a-f792-6cb829d53032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-01 12:33:43--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
            "Resolving download.microsoft.com (download.microsoft.com)... 23.0.194.53, 2600:1409:3c00:c8c::317f, 2600:1409:3c00:c80::317f\n",
            "Connecting to download.microsoft.com (download.microsoft.com)|23.0.194.53|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 824887076 (787M) [application/octet-stream]\n",
            "Saving to: ‘kagglecatsanddogs_5340.zip’\n",
            "\n",
            "kagglecatsanddogs_5 100%[===================>] 786.67M   274MB/s    in 2.9s    \n",
            "\n",
            "2026-02-01 12:33:46 (274 MB/s) - ‘kagglecatsanddogs_5340.zip’ saved [824887076/824887076]\n",
            "\n",
            "--2026-02-01 12:33:46--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  47.4MB/s    in 1.7s    \n",
            "\n",
            "2026-02-01 12:33:48 (47.4 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "--2026-02-01 12:33:57--  https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/prep_data.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1912 (1.9K) [text/plain]\n",
            "Saving to: ‘prep_data.py’\n",
            "\n",
            "prep_data.py        100%[===================>]   1.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-01 12:33:57 (41.0 MB/s) - ‘prep_data.py’ saved [1912/1912]\n",
            "\n",
            "Unpacking CatsvsDogs\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        }
      ],
      "source": [
        "### Download Dogs vs Cats Zip ###\n",
        "!wget https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\n",
        "\n",
        "### Download IMDB Dataset ###\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "\n",
        "### Clean-Up CatsVDogs Images\n",
        "!wget https://raw.githubusercontent.com/priyammaz/HAL-DL-From-Scratch/main/prep_data.py\n",
        "!python -m prep_data --catsvdogs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images live on disk → paths are collected → labels are assigned → PyTorch samples by index → images are loaded one at a time when needed(lazy loading) ."
      ],
      "metadata": {
        "id": "4pL-HDZTh7sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder # Stream data from images stored in folders\n",
        "\n",
        "import os # Allows to access files\n",
        "import numpy as np\n",
        "from PIL import Image # Allows us to Load Images\n",
        "from collections import Counter # Utility function to give us the counts of unique items in an iterable"
      ],
      "metadata": {
        "id": "T4tgCGYmBk_F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_folder = \"/content/PetImages/\"\n",
        "os.listdir(path_to_folder)"
      ],
      "metadata": {
        "id": "kMBqJ298Bk7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07caff04-5824-4688-bd2c-481fe5ace2de"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cat', 'Dog']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "class DogsVsCats(Dataset):\n",
        "\n",
        "    def __init__(self, path_to_folder):\n",
        "        # Build paths to class folders\n",
        "        path_to_cats = os.path.join(path_to_folder, \"Cat\")\n",
        "        path_to_dogs = os.path.join(path_to_folder, \"Dog\")\n",
        "\n",
        "        # List all image filenames\n",
        "        dog_files = os.listdir(path_to_dogs)\n",
        "        cat_files = os.listdir(path_to_cats)\n",
        "\n",
        "        # Build full paths\n",
        "        path_to_dog_files = [\n",
        "            os.path.join(path_to_dogs, file) for file in dog_files\n",
        "        ]\n",
        "        path_to_cat_files = [\n",
        "            os.path.join(path_to_cats, file) for file in cat_files\n",
        "        ]\n",
        "\n",
        "        # Combine all image paths\n",
        "        self.training_files = path_to_dog_files + path_to_cat_files\n",
        "\n",
        "        # Labels\n",
        "        self.dog_label = 0\n",
        "        self.cat_label = 1\n",
        "\n",
        "        # Image → Tensor transform\n",
        "        self.transform = transforms.ToTensor()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image path\n",
        "        path_to_image = self.training_files[idx]\n",
        "\n",
        "        # Assign label based on folder name\n",
        "        if \"Dog\" in path_to_image:\n",
        "            label = self.dog_label\n",
        "        else:\n",
        "            label = self.cat_label\n",
        "\n",
        "        # Load image and convert to tensor\n",
        "        image = Image.open(path_to_image).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "hpA1F5XQBk2f"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_folder = \"/content/PetImages\"\n",
        "dogvcat = DogsVsCats(path_to_folder)\n",
        "\n",
        "print(\"Total Training Samples:\", len(dogvcat))\n",
        "\n",
        "image, label = dogvcat[0]\n",
        "print(\"Label:\", label)\n",
        "print(\"Image Shape:\", image.shape)\n"
      ],
      "metadata": {
        "id": "zsvv3YskBkz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842e9938-bb38-4a4a-85c9-79d13fb22501"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Samples: 24998\n",
            "Label: 0\n",
            "Image Shape: torch.Size([3, 500, 375])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Create a Composition of Transformations\n",
        "img_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((224,224)), # Resize the image from whatever it is to a [3, 224, 224] image\n",
        "        transforms.RandomHorizontalFlip(p=0.5), # Do a random flip with 50% probability\n",
        "        transforms.ToTensor(), # Convert the image to a Tensor\n",
        "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]) # Normalize the Image\n",
        "    ]\n",
        ")\n",
        "\n",
        "### Lets Add this to our Dataset Class\n",
        "class DogsVsCats(Dataset):\n",
        "    def __init__(self, path_to_folder, transforms): ### Our INIT Now Accepts a Transforms\n",
        "\n",
        "        ### PREVIOUS CODE ###\n",
        "        path_to_cats = os.path.join(path_to_folder, \"Cat\")\n",
        "        path_to_dogs = os.path.join(path_to_folder, \"Dog\")\n",
        "        dog_files = os.listdir(path_to_dogs)\n",
        "        cat_files = os.listdir(path_to_cats)\n",
        "        path_to_dog_files = [os.path.join(path_to_dogs, file) for file in dog_files]\n",
        "        path_to_cat_files = [os.path.join(path_to_cats, file) for file in cat_files]\n",
        "        self.training_files = path_to_dog_files + path_to_cat_files\n",
        "        self.dog_label, self.cat_label = 0, 1\n",
        "\n",
        "\n",
        "        ### NEW CODE ###\n",
        "        # self.transform = transforms.ToTensor() -> Notice how our transforms was just ToTensor before. It will be our Composition of Transforms now!\n",
        "        self.transform = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.training_files) # The number of samples we have is just the number of training files we have\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ### PREVIOUS CODE ###\n",
        "        path_to_image = self.training_files[idx] # Grab file path at the sampled index\n",
        "        if \"Dog\" in path_to_image: # If the word \"Dog\" is in the filepath, then set the label to 0\n",
        "            label = self.dog_label\n",
        "        else:\n",
        "            label = self.cat_label # Otherwise set the label to 1\n",
        "        image = Image.open(path_to_image) # Open Image with PIL to create a PIL Image\n",
        "\n",
        "        ### UPDATED CODE ###\n",
        "        image = self.transform(image) # Image now will go through series of transforms indicated in self.transform\n",
        "        return image, label\n",
        "\n",
        "\n",
        "### Instantiate Dataset With the Transforms ###\n",
        "dogvcat = DogsVsCats(path_to_folder=\"PetImages/\",\n",
        "                     transforms=img_transforms)\n",
        "\n",
        "\n",
        "dogsvcatsloader = DataLoader(dogvcat,\n",
        "                             batch_size=16,\n",
        "                             shuffle=True)\n",
        "\n",
        "for images, labels in dogsvcatsloader:\n",
        "    print(images.shape)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "id": "p46UkXliBkw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eedb29e-7fea-4c8e-bb6a-f50ffea8c325"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 3, 224, 224])\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_samples = int(0.9 * len(dogvcat))\n",
        "test_samples = len(dogvcat) - train_samples\n",
        "\n",
        "print(\"Number of Training Samples:\", train_samples, \"Number of Test Samples\", test_samples)\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dogvcat, lengths=[train_samples, test_samples]) # Split with the dataset by the lenghts\n",
        "\n",
        "### Load Datasets into two DataLoaders ###\n",
        "trainloader = DataLoader(train_dataset,\n",
        "                         batch_size=16,\n",
        "                         shuffle=True)\n",
        "\n",
        "testloader = DataLoader(test_dataset,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True)\n",
        "\n",
        "\n",
        "### Test Loaders ###\n",
        "for images, labels in trainloader:\n",
        "    print(images.shape)\n",
        "    print(labels)\n",
        "    break\n",
        "\n",
        "for images, labels in testloader:\n",
        "    print(images.shape)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "id": "j-fn6ucvBkuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6207ed86-ca3b-4b07-c56f-c239b2286349"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Samples: 22498 Number of Test Samples 2500\n",
            "torch.Size([16, 3, 224, 224])\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "torch.Size([16, 3, 224, 224])\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rL53156eBkrI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J37pGSs-Bknt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4V0ztb0LBklD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "khh2r_vPBkiN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qX8W2f2CBkfW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVuW2XiBBkci"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v3PGfnn8BkaC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vrlp74mNBkXR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ARrGd-aeBkUs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYTOFJp6BkSC"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}